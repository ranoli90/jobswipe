# fly.toml app configuration file generated for jobswipe-9obhra on 2026-01-27T06:18:52Z
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'jobswipe-9obhra'
primary_region = 'iad'

[build]

[env]
  PORT = "8080"

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 2
  max_machines_running = 10
  processes = ['app']
  
  [http_service.concurrency]
    type = "connections"
    hard_limit = 25
    soft_limit = 20
    
  [[http_service.checks]]
    interval = "15s"
    timeout = "10s"
    grace_period = "5s"
    method = "GET"
    path = "/health"

  [[http_service.scale_to_zero]]
    grace_period = "300s" # 5 minutes

[processes]
  app = "python -m uvicorn backend.api.main:app --host 0.0.0.0 --port 8080"
  worker = "cd /app/backend && celery -A workers.celery_app worker --loglevel=info --concurrency=4"

[[vm]]
  memory = '2gb'
  cpu_kind = 'shared'
  cpus = 2

[metrics]
  port = 9091
  path = "/metrics"

# Auto-scaling configuration based on metrics
[experimental]
  auto_scaling = true

[[http_service.scale]]
  # Scale based on CPU utilization
  metric_name = "cpu"
  metric_type = "system"
  threshold = 70
  unit = "percent"
  adjustment = "+1"
  cooldown = "60s"

[[http_service.scale]]
  # Scale based on memory utilization
  metric_name = "memory"
  metric_type = "system"
  threshold = 80
  unit = "percent"
  adjustment = "+1"
  cooldown = "60s"

[[http_service.scale]]
  # Scale based on request queue length (Prometheus metric)
  metric_name = "request_queue_length"
  metric_type = "prometheus"
  threshold = 50
  unit = "count"
  adjustment = "+1"
  cooldown = "30s"

[[http_service.scale]]
  # Scale down when CPU is low
  metric_name = "cpu"
  metric_type = "system"
  threshold = 30
  unit = "percent"
  adjustment = "-1"
  cooldown = "300s" # 5 minutes
