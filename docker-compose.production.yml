version: '3.8'

services:
  # Database
  postgres:
    image: postgres:15
    container_name: jobswipe-postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Redis for caching and Celery broker
  redis:
    image: redis:7-alpine
    container_name: jobswipe-redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.1'

  # RabbitMQ for Celery message broker
  rabbitmq:
    image: rabbitmq:3-management
    container_name: jobswipe-rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS}
    healthcheck:
      test: ["CMD-SHELL", "rabbitmq-diagnostics -q ping"]
      interval: 30s
      timeout: 30s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Ollama for AI services
  ollama:
    image: ollama/ollama:latest
    container_name: jobswipe-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    command: serve
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '1.0'
        reservations:
          memory: 2G
          cpus: '0.5'

  # Backend API
  backend:
    build: ./backend
    container_name: jobswipe-backend
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      CELERY_BROKER_URL: ${CELERY_BROKER_URL}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}
      SECRET_KEY: ${SECRET_KEY}
      ENCRYPTION_PASSWORD: ${ENCRYPTION_PASSWORD}
      ENCRYPTION_SALT: ${ENCRYPTION_SALT}
      VAULT_URL: ${VAULT_URL}
      VAULT_TOKEN: ${VAULT_TOKEN}
      OAUTH2_REDIRECT_URI: ${OAUTH2_REDIRECT_URI}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
      OLLAMA_MODEL: ${OLLAMA_MODEL}
      OLLAMA_EMBEDDING_MODEL: ${OLLAMA_EMBEDDING_MODEL}
      LOG_LEVEL: ${LOG_LEVEL}
      ENVIRONMENT: production
      DEBUG: "False"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      - ./backend:/app
    command: >
      sh -c "
        # Wait for PostgreSQL
        while ! nc -z postgres 5432; do
          echo 'PostgreSQL is unavailable - sleeping'
          sleep 1
        done
        echo 'PostgreSQL is up - continuing'

        # Run database migrations
        echo "Running database migrations..."
        alembic upgrade head

        # Initialize database (fallback for any remaining setup)
        python -c 'from db.database import init_db; init_db()'

        # Start API
        uvicorn api.main:app --host 0.0.0.0 --port 8000
      "
    healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:8000/ready"]
       interval: 30s
       timeout: 10s
       retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Celery worker for application automation
  celery-worker:
    build: ./backend
    container_name: jobswipe-celery-worker
    environment:
      DATABASE_URL: ${DATABASE_URL}
      CELERY_BROKER_URL: ${CELERY_BROKER_URL}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}
      SECRET_KEY: ${SECRET_KEY}
      ENCRYPTION_PASSWORD: ${ENCRYPTION_PASSWORD}
      ENCRYPTION_SALT: ${ENCRYPTION_SALT}
      VAULT_URL: ${VAULT_URL}
      VAULT_TOKEN: ${VAULT_TOKEN}
      OAUTH2_REDIRECT_URI: ${OAUTH2_REDIRECT_URI}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
      OLLAMA_MODEL: ${OLLAMA_MODEL}
      OLLAMA_EMBEDDING_MODEL: ${OLLAMA_EMBEDDING_MODEL}
      LOG_LEVEL: ${LOG_LEVEL}
      ENVIRONMENT: production
      DEBUG: "False"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      - ./backend:/app
      - ./tools:/app/tools
    command: >
      sh -c "
        # Wait for services
        while ! nc -z rabbitmq 5672; do
          echo 'RabbitMQ is unavailable - sleeping'
          sleep 1
        done

        # Start worker
        celery -A workers.app_agent_worker worker --loglevel=info --concurrency=4
      "
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Celery Flower for monitoring (optional in production)
  celery-flower:
    build: ./backend
    container_name: jobswipe-celery-flower
    ports:
      - "5555:5555"
    environment:
      CELERY_BROKER_URL: ${CELERY_BROKER_URL}
    depends_on:
      rabbitmq:
        condition: service_healthy
    command: >
      sh -c "
        while ! nc -z rabbitmq 5672; do
          echo 'RabbitMQ is unavailable - sleeping'
          sleep 1
        done

        celery -A workers.app_agent_worker flower --port=5555 --broker=${CELERY_BROKER_URL}
      "
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.1'

  # OpenSearch for search and analytics
  opensearch:
    image: opensearchproject/opensearch:2.11.0
    container_name: jobswipe-opensearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      - "DISABLE_SECURITY_PLUGIN=true"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # OpenSearch Dashboards for visualization
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.0
    container_name: jobswipe-opensearch-dashboards
    ports:
      - "5601:5601"
    environment:
      OPENSEARCH_HOSTS: '["http://opensearch:9200"]'
    depends_on:
      opensearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: jobswipe-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./tools/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Grafana for dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: jobswipe-grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_INSTALL_PLUGINS: "grafana-piechart-panel"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./tools/grafana/provisioning:/etc/grafana/provisioning
      - ./tools/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jobswipe-jaeger
    ports:
      - "16686:16686"
      - "14268:14268"
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:16686"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'


volumes:
  postgres_data:
  opensearch_data:
  grafana_data:
  ollama_data: